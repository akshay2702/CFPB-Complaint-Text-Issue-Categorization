{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"CFPB_Complain_text_Contextual_topic-modeling-bert-lda_3010.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"}},"cells":[{"cell_type":"code","metadata":{"id":"w0NKfk2q9bpO","executionInfo":{"status":"ok","timestamp":1604053651376,"user_tz":-330,"elapsed":18816,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"b6a6d850-2078-4067-aaa5-b6bf1e9c0c1e","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install spacy-langdetect\n","!pip install language-detector\n","!pip install symspellpy\n","!pip install sentence-transformers\n","!pip install --upgrade gensim\n","!pip install pyLDAvis3\n","!pip install stop-words\n","!python -m pip install -U pyLDAvis\n","import nltk\n","nltk.download('averaged_perceptron_tagger')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('words')\n","nltk.download('punkt')"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy-langdetect in /usr/local/lib/python3.6/dist-packages (0.1.2)\n","Requirement already satisfied: langdetect==1.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy-langdetect) (1.0.7)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from spacy-langdetect) (3.6.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect==1.0.7->spacy-langdetect) (1.15.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.4.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (20.2.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (8.5.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (50.3.2)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (1.9.0)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->spacy-langdetect) (0.7.1)\n","Requirement already satisfied: language-detector in /usr/local/lib/python3.6/dist-packages (5.0.2)\n","Requirement already satisfied: symspellpy in /usr/local/lib/python3.6/dist-packages (6.7.0)\n","Requirement already satisfied: numpy>=1.13.1 in /usr/local/lib/python3.6/dist-packages (from symspellpy) (1.18.5)\n","Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.6/dist-packages (0.3.8)\n","Requirement already satisfied: torch>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.6.0+cu101)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (4.41.1)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (3.2.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (0.22.2.post1)\n","Collecting transformers<3.4.0,>=3.1.0\n","  Using cached https://files.pythonhosted.org/packages/19/22/aff234f4a841f8999e68a7a94bdd4b60b4cebcfeca5d67d61cd08c9179de/transformers-3.3.1-py3-none-any.whl\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.18.5)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from sentence-transformers) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.2.0->sentence-transformers) (0.16.0)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk->sentence-transformers) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->sentence-transformers) (0.17.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2.23.0)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (20.4)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.0.43)\n","Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.8.1rc2)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.12)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (2019.12.20)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.7)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers<3.4.0,>=3.1.0->sentence-transformers) (0.1.94)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers<3.4.0,>=3.1.0->sentence-transformers) (2020.6.20)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers<3.4.0,>=3.1.0->sentence-transformers) (2.4.7)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers<3.4.0,>=3.1.0->sentence-transformers) (7.1.2)\n","Installing collected packages: transformers\n","  Found existing installation: transformers 2.2.2\n","    Uninstalling transformers-2.2.2:\n","      Successfully uninstalled transformers-2.2.2\n","Successfully installed transformers-3.3.1\n","Requirement already up-to-date: gensim in /usr/local/lib/python3.6/dist-packages (3.8.3)\n","Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.5)\n","Requirement already satisfied, skipping upgrade: smart-open>=1.8.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (3.0.0)\n","Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied, skipping upgrade: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.15.0)\n","Requirement already satisfied, skipping upgrade: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.8.1->gensim) (2.23.0)\n","Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (3.0.4)\n","Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2.10)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (2020.6.20)\n","Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.8.1->gensim) (1.24.3)\n","\u001b[31mERROR: Could not find a version that satisfies the requirement pyLDAvis3 (from versions: none)\u001b[0m\n","\u001b[31mERROR: No matching distribution found for pyLDAvis3\u001b[0m\n","Requirement already satisfied: stop-words in /usr/local/lib/python3.6/dist-packages (2018.7.23)\n","Requirement already up-to-date: pyLDAvis in /usr/local/lib/python3.6/dist-packages (2.1.2)\n","Requirement already satisfied, skipping upgrade: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.35.1)\n","Requirement already satisfied, skipping upgrade: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.17.0)\n","Requirement already satisfied, skipping upgrade: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.2)\n","Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Requirement already satisfied, skipping upgrade: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.1.3)\n","Requirement already satisfied, skipping upgrade: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied, skipping upgrade: funcy in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.15)\n","Requirement already satisfied, skipping upgrade: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n","Requirement already satisfied, skipping upgrade: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied, skipping upgrade: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.5)\n","Requirement already satisfied, skipping upgrade: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n","Requirement already satisfied, skipping upgrade: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n","Requirement already satisfied, skipping upgrade: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n","Requirement already satisfied, skipping upgrade: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.9.0)\n","Requirement already satisfied, skipping upgrade: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Requirement already satisfied, skipping upgrade: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (20.2.0)\n","Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (50.3.2)\n","Requirement already satisfied, skipping upgrade: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.5.0)\n","Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.15.0)\n","Requirement already satisfied, skipping upgrade: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.4.0)\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n","[nltk_data]       date!\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package words to /root/nltk_data...\n","[nltk_data]   Package words is already up-to-date!\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"code","metadata":{"id":"M4hcZvMB9bqs","executionInfo":{"status":"ok","timestamp":1604053651383,"user_tz":-330,"elapsed":18794,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["#importing all dependencies.\n","import os\n","import json\n","import pandas as pd\n","from tqdm import tqdm\n","import numpy as np\n","from nltk.corpus import wordnet\n","import re\n","import matplotlib.pyplot as plt\n","from nltk.corpus import stopwords \n","from nltk.corpus import wordnet\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer\n","from nltk.stem.porter import PorterStemmer\n","from nltk.tokenize import word_tokenize\n","from nltk.tokenize import sent_tokenize\n","from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"],"execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QR-MNU6Q9bqH"},"source":["# Topic Modeling with BERT, LDA, and Clustering\n","#### Latent Dirichlet Allocation (LDA) probabilistic topic assignment and pre-trained sentence embeddings from BERT/RoBERTa"]},{"cell_type":"markdown","metadata":{"id":"McaZB_ls9bqZ"},"source":["## Model Deep Dive\n","\n","The author used: \n","\n","* LDA for probabilistic topic assignment vector.\n","* Bert for sentence embedding vector.\n","\n","1. Concatenated both LDA and Bert vectors with a weight hyperparameter to balance the relative importance of information from each source.\n","2. Used autoencoder to learn a lower dimensional latent space representation of the concatenated vector.\n","\n","* The assumption is that the concatendate vector shoul have a manifold shaep in the high dimensional space. \n","* USed clustering on the latent space representations to get topics. \n","\n"]},{"cell_type":"markdown","metadata":{"id":"NMNDqOpo9bqb"},"source":["![Contextual Topic Identification model design](https://miro.medium.com/max/1410/1*OKCYnB-JbGq1NDwNSKd5Zw.png)"]},{"cell_type":"code","metadata":{"id":"Wfjewcsj_WyO","executionInfo":{"status":"ok","timestamp":1604053651386,"user_tz":-330,"elapsed":18781,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"1339ea8b-0a8a-429a-9198-9b4ab9a5937c","colab":{"base_uri":"https://localhost:8080/"}},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"-K0sF2HZooSr","executionInfo":{"status":"ok","timestamp":1604053652730,"user_tz":-330,"elapsed":20103,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["df= pd.read_excel(r'/content/drive/My Drive/CFPB LDA/LDA Output Document to topic.xlsx')\n","data = df['Cleansed_Text']"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"vIAZAe9dZt8j","executionInfo":{"status":"ok","timestamp":1604053652738,"user_tz":-330,"elapsed":20095,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"91e5be28-50ae-469e-d568-a5d5fec484e2","colab":{"base_uri":"https://localhost:8080/","height":105}},"source":["'''import pandas as pd\n","df1= pd.read_csv(r'/content/drive/My Drive/CFPB LDA/Mortgage_complaints_text_test.csv')\n","df2= pd.read_excel(r'/content/drive/My Drive/CFPB LDA/CFPB_text_vs_cnsd_text_20201013_08_05_41.xlsx')\n","df1 = df1.rename(columns={'Complaint ID':'Complaint_ID'})\n","print(df1.columns)\n","print(df2.columns)\n","df = pd.merge(df2[['Complaint_ID', 'Complain_text', 'Modified_text']], df1[['Complaint_ID','Issue','Text_Length']], on=['Complaint_ID'])\n","df[\"Modified_text\"] = df[\"Modified_text\"].replace(np.nan, '', regex=True)'''\n","#df.info()"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'import pandas as pd\\ndf1= pd.read_csv(r\\'/content/drive/My Drive/CFPB LDA/Mortgage_complaints_text_test.csv\\')\\ndf2= pd.read_excel(r\\'/content/drive/My Drive/CFPB LDA/CFPB_text_vs_cnsd_text_20201013_08_05_41.xlsx\\')\\ndf1 = df1.rename(columns={\\'Complaint ID\\':\\'Complaint_ID\\'})\\nprint(df1.columns)\\nprint(df2.columns)\\ndf = pd.merge(df2[[\\'Complaint_ID\\', \\'Complain_text\\', \\'Modified_text\\']], df1[[\\'Complaint_ID\\',\\'Issue\\',\\'Text_Length\\']], on=[\\'Complaint_ID\\'])\\ndf[\"Modified_text\"] = df[\"Modified_text\"].replace(np.nan, \\'\\', regex=True)'"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"zr01gj6AukPA"},"source":["# summa – textrank"]},{"cell_type":"markdown","metadata":{"id":"GMu_6-6kup1P"},"source":["* Text summarization"]},{"cell_type":"code","metadata":{"id":"TBU95Hh_r0Y1","executionInfo":{"status":"ok","timestamp":1604053654944,"user_tz":-330,"elapsed":22283,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"7936b9e6-1cfd-4218-db4b-2781febcc144","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install summa\n","from summa import summarizer"],"execution_count":8,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: summa in /usr/local/lib/python3.6/dist-packages (1.2.0)\n","Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from summa) (1.4.1)\n","Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=0.19->summa) (1.18.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YCSfPTg5rPu3","executionInfo":{"status":"ok","timestamp":1604053654948,"user_tz":-330,"elapsed":22269,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"eafbb4a8-f7ea-4fd6-8e51-e5c7469a596b","colab":{"base_uri":"https://localhost:8080/"}},"source":["text1 = df['Original_Text'][0]\n","text2 = df['Cleansed_Text'][0]\n","print('Orginal Text: ',text1)\n","print('Summarize Text: ',summarizer.summarize(text1))\n","print('Orginal Text: ',text2)\n","print('Summarize Text: ',summarizer.summarize(text2))"],"execution_count":9,"outputs":[{"output_type":"stream","text":["Orginal Text:  Greetings, Nationstar mortgage contacted me saying that my association no longer had valid flood insurance. I checked with my association and it turns out we no longer needed flood insurance for my condo. I got the required \" Standard Flood Hazard Determination '' document from my condo stating that the condo was no longer in a flood zone. I uploaded the document per Nationstar instructions and also faxed it. A month letter I got another letter. I called and they said they did get the documents but they were backed up and would fix it. I received at least XXXX more letters and called XXXX more times. Each time I was told this would be fixed quickly. Finally they took out an escrow to pay for the flood insurance I did n't need. I called and they showed that part of the insurance had been refunded but not all. Again, they said they would fix.\n","Summarize Text:  I checked with my association and it turns out we no longer needed flood insurance for my condo.\n","I called and they said they did get the documents but they were backed up and would fix it.\n","Orginal Text:  greeting mortgage contact me say that my association longer have valid flood insurance check with my association and it turn out we long need flood insurance my get the require document from my state that the longer flood zone the document per instruction and also it month letter get another letter call and they say they do get the document but they back up and would fix it receive at least more letter and call more time each time tell this would fix quickly final they take out an escrow payment the flood insurance do need call and they show that part the insurance have refund but not all again they say they would fix\n","Summarize Text:  \n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"o1-kr9d3uxiq"},"source":["* Keyword extraction"]},{"cell_type":"code","metadata":{"id":"BLWqi36OvBE1","executionInfo":{"status":"ok","timestamp":1604053654951,"user_tz":-330,"elapsed":22255,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from summa import keywords"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"IMjV0abrvF0U","executionInfo":{"status":"ok","timestamp":1604053654952,"user_tz":-330,"elapsed":22245,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"6423b199-6961-47ab-e4e1-51e2ecbf7aac","colab":{"base_uri":"https://localhost:8080/"}},"source":["print('Orginal Text: ',text1)\n","print('Keywords of Text: ',keywords.keywords(text1))\n","print('Orginal Text: ',text2)\n","print('Keywords of Text: ',keywords.keywords(text2))"],"execution_count":11,"outputs":[{"output_type":"stream","text":["Orginal Text:  Greetings, Nationstar mortgage contacted me saying that my association no longer had valid flood insurance. I checked with my association and it turns out we no longer needed flood insurance for my condo. I got the required \" Standard Flood Hazard Determination '' document from my condo stating that the condo was no longer in a flood zone. I uploaded the document per Nationstar instructions and also faxed it. A month letter I got another letter. I called and they said they did get the documents but they were backed up and would fix it. I received at least XXXX more letters and called XXXX more times. Each time I was told this would be fixed quickly. Finally they took out an escrow to pay for the flood insurance I did n't need. I called and they showed that part of the insurance had been refunded but not all. Again, they said they would fix.\n","Keywords of Text:  nationstar mortgage contacted\n","instructions\n","greetings\n","Orginal Text:  greeting mortgage contact me say that my association longer have valid flood insurance check with my association and it turn out we long need flood insurance my get the require document from my state that the longer flood zone the document per instruction and also it month letter get another letter call and they say they do get the document but they back up and would fix it receive at least more letter and call more time each time tell this would fix quickly final they take out an escrow payment the flood insurance do need call and they show that part the insurance have refund but not all again they say they would fix\n","Keywords of Text:  greeting mortgage contact\n","flood\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bi3jgQfWvYaT"},"source":["# Bert Extractive Summarizer"]},{"cell_type":"code","metadata":{"id":"Sf3XwkQpvrJo","executionInfo":{"status":"ok","timestamp":1604053657171,"user_tz":-330,"elapsed":24448,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"a0be9b51-a2f2-4597-9f09-be839341c4ef","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install bert-extractive-summarizer"],"execution_count":12,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: bert-extractive-summarizer in /usr/local/lib/python3.6/dist-packages (0.5.1)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (3.3.1)\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (2.1.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from bert-extractive-summarizer) (0.22.2.post1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (2019.12.20)\n","Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.1.94)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (2.23.0)\n","Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (1.18.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.0.43)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (3.0.12)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (4.41.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (20.4)\n","Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers->bert-extractive-summarizer) (0.8.1rc2)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.0.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.2)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.2.4)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (1.0.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.9.6)\n","Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.6.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (0.8.0)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (2.0.1)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy->bert-extractive-summarizer) (7.0.8)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (0.17.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn->bert-extractive-summarizer) (1.4.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers->bert-extractive-summarizer) (3.0.4)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers->bert-extractive-summarizer) (7.1.2)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers->bert-extractive-summarizer) (2.4.7)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"1y75u3zlt1h9","executionInfo":{"status":"ok","timestamp":1604053665769,"user_tz":-330,"elapsed":33029,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"1dc07bf4-c628-410b-b9cf-c9a86d2c9a08","colab":{"base_uri":"https://localhost:8080/"}},"source":["!pip install spacy==2.1.3\n","!pip install transformers==2.2.2\n","!pip install neuralcoref\n","\n","!python -m spacy download en_core_web_md"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: spacy==2.1.3 in /usr/local/lib/python3.6/dist-packages (2.1.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.18.5)\n","Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.6.0)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.1)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.8.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (1.0.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.9.6)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (0.2.4)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.23.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (2.0.3)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy==2.1.3) (7.0.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.1.3) (1.24.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy==2.1.3) (4.41.1)\n","Collecting transformers==2.2.2\n","  Using cached https://files.pythonhosted.org/packages/d1/08/4a6768ca1a7a4fa37e5ee08077c5d02b8d83876bd36caa5fc24d98992ac2/transformers-2.2.2-py3-none-any.whl\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (2019.12.20)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (1.18.5)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (1.16.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (4.41.1)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (0.0.43)\n","Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.2.2) (0.1.94)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (2020.6.20)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.2.2) (1.24.3)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (0.3.3)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (0.10.0)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.8 in /usr/local/lib/python3.6/dist-packages (from boto3->transformers==2.2.2) (1.19.8)\n","Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (1.15.0)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.2.2) (0.17.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.8->boto3->transformers==2.2.2) (2.8.1)\n","\u001b[31mERROR: sentence-transformers 0.3.8 has requirement transformers<3.4.0,>=3.1.0, but you'll have transformers 2.2.2 which is incompatible.\u001b[0m\n","Installing collected packages: transformers\n","  Found existing installation: transformers 3.3.1\n","    Uninstalling transformers-3.3.1:\n","      Successfully uninstalled transformers-3.3.1\n","Successfully installed transformers-2.2.2\n","Requirement already satisfied: neuralcoref in /usr/local/lib/python3.6/dist-packages (4.0)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.16.8)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (1.18.5)\n","Requirement already satisfied: spacy>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.1.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from neuralcoref) (2.23.0)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.3.3)\n","Requirement already satisfied: botocore<1.20.0,>=1.19.8 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (1.19.8)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->neuralcoref) (0.10.0)\n","Requirement already satisfied: srsly<1.1.0,>=0.0.5 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n","Requirement already satisfied: blis<0.3.0,>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.2.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (1.0.2)\n","Requirement already satisfied: wasabi<1.1.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.8.0)\n","Requirement already satisfied: thinc<7.1.0,>=7.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (7.0.8)\n","Requirement already satisfied: preshed<2.1.0,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.0.1)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (0.9.6)\n","Requirement already satisfied: jsonschema<3.0.0,>=2.6.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.1.0->neuralcoref) (2.6.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2020.6.20)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->neuralcoref) (2.10)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.20.0,>=1.19.8->boto3->neuralcoref) (2.8.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<7.1.0,>=7.0.2->spacy>=2.1.0->neuralcoref) (4.41.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.20.0,>=1.19.8->boto3->neuralcoref) (1.15.0)\n","Requirement already satisfied: en_core_web_md==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.1.0/en_core_web_md-2.1.0.tar.gz#egg=en_core_web_md==2.1.0 in /usr/local/lib/python3.6/dist-packages (2.1.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_md')\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"U7x5KWg6v5ft"},"source":["#### Simple Example"]},{"cell_type":"code","metadata":{"id":"Iaf8h9studY2","executionInfo":{"status":"error","timestamp":1604053678147,"user_tz":-330,"elapsed":45390,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"1ab6d249-6bb9-47da-83ee-63ff4ddcd686","colab":{"base_uri":"https://localhost:8080/","height":383}},"source":["from summarizer import Summarizer\n","\n","model = Summarizer()\n","print('Orginal Text: ',text1)\n","print('Summarize Text: ',model(text1))\n","print('Orginal Text: ',text2)\n","print('Summarize Text: ',model(text2))\n","\n","#Specifying number of sentences\n","result = model(text1, ratio=0.2)  # Specified with ratio\n","print('Summarize Text: ',model(text1))\n","result = model(text1, num_sentences=3)\n","print('Summarize Text: ',model(result))"],"execution_count":14,"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-978f9d71af28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msummarizer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSummarizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Orginal Text: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Summarize Text: '\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/summarizer/model_processors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         super(Summarizer, self).__init__(\n\u001b[0;32m--> 263\u001b[0;31m             \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentence_handler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         )\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/summarizer/model_processors.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, custom_model, custom_tokenizer, hidden, reduce_option, sentence_handler, random_state)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertParent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce_option\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce_option\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/summarizer/bert_parent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, custom_model, custom_tokenizer)\u001b[0m\n\u001b[1;32m     42\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcustom_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcustom_tokenizer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    407\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfrom_tf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolved_archive_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mmissing_keys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    583\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_legacy_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    586\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    770\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_storage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 772\u001b[0;31m         \u001b[0mdeserialized_objects\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf_should_read_directly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    773\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0moffset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0moffset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"bAW2Qe3gwyxt","executionInfo":{"status":"aborted","timestamp":1604053678094,"user_tz":-330,"elapsed":45321,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["result = model(text1, min_length=60)\n","full = ''.join(result)\n","print(full)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"a_aAnrqt0MrJ"},"source":["# PyTLDR: Automatic Text Summarization in Python"]},{"cell_type":"code","metadata":{"id":"fsh7mhSB0Q9U","executionInfo":{"status":"aborted","timestamp":1604053678099,"user_tz":-330,"elapsed":45309,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["!pip install git+https://github.com/jaijuneja/PyTLDR.git"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oKnIR3p00zXY","executionInfo":{"status":"aborted","timestamp":1604053678104,"user_tz":-330,"elapsed":45297,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from pytldr.summarize import TextRankSummarizer\n","from pytldr.nlp import Tokenizer\n","\n","tokenizer = Tokenizer('english')\n","summarizer = TextRankSummarizer(tokenizer)\n","\n","# If you don't specify a tokenizer when intiializing a summarizer then the\n","# English tokenizer will be used by default\n","summarizer = TextRankSummarizer()  # English tokenizer used\n","\n","# This object creates a summary using the summarize method:\n","# e.g. summarizer.summarize(text, length=5, weighting='frequency', norm=None)\n","\n","# The length parameter specifies the length of the summary, either as a\n","# number of sentences, or a percentage of the original text\n","\n","# The summarizer can take as input...\n","# 1. A string:\n","summary = summarizer.summarize(text1, length=4)\n","print(summary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_MUDdWJvCkc4"},"source":["# Latent Semantic Analysis (LSA) Summarization"]},{"cell_type":"code","metadata":{"id":"fJCanOJwCrE9","executionInfo":{"status":"aborted","timestamp":1604053678108,"user_tz":-330,"elapsed":45285,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from pytldr.summarize import LsaSummarizer, LsaOzsoy, LsaSteinberger\n","\n","summarizer = LsaOzsoy()\n","summarizer = LsaSteinberger()\n","summarizer = LsaSummarizer()  # This is identical to the LsaOzsoy object\n","\n","summary = summarizer.summarize(\n","    text, topics=4, length=5, binary_matrix=True, topic_sigma_threshold=0.5\n",")\n","print(summary)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MunXX8H2Eljk"},"source":["# Implementing Text Summarization in Python using Keras"]},{"cell_type":"code","metadata":{"id":"Cnf0hdorErSk","executionInfo":{"status":"aborted","timestamp":1604053678110,"user_tz":-330,"elapsed":45279,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from attention import AttentionLayer"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mX4B6gf3Etti","executionInfo":{"status":"aborted","timestamp":1604053678112,"user_tz":-330,"elapsed":45275,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["import numpy as np  \n","import pandas as pd \n","import re           \n","from bs4 import BeautifulSoup \n","from keras.preprocessing.text import Tokenizer \n","from keras.preprocessing.sequence import pad_sequences\n","from nltk.corpus import stopwords   \n","from tensorflow.keras.layers import Input, LSTM, Embedding, Dense, Concatenate, TimeDistributed, Bidirectional\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.callbacks import EarlyStopping\n","import warnings\n","pd.set_option(\"display.max_colwidth\", 200)\n","warnings.filterwarnings(\"ignore\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MZQcIxTf-9PW"},"source":["## Texts summarizing with the help of Spacy"]},{"cell_type":"code","metadata":{"id":"pDY78qq4AY3d","executionInfo":{"status":"aborted","timestamp":1604053678114,"user_tz":-330,"elapsed":45268,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["#!pip install spacy\n","#!python -m spacy download en_core_web_lg\n","import spacy\n","nlp = spacy.load('en_core_web_lg')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rASIHRh4_BED","executionInfo":{"status":"ok","timestamp":1604053763873,"user_tz":-330,"elapsed":978,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["import heapq\n","# this is function for text summarization\n","\n","def generate_summary(text_without_removing_dot, cleaned_text):\n","    sample_text = text_without_removing_dot\n","    doc = nlp(sample_text)\n","    sentence_list=[]\n","    for idx, sentence in enumerate(doc.sents): # we are using spacy for sentence tokenization\n","        sentence_list.append(re.sub(r'[^\\w\\s]','',str(sentence)))\n","\n","    stopwords = nltk.corpus.stopwords.words('english')\n","\n","    word_frequencies = {}  \n","    for word in nltk.word_tokenize(cleaned_text):  \n","        if word not in stopwords:\n","            if word not in word_frequencies.keys():\n","                word_frequencies[word] = 1\n","            else:\n","                word_frequencies[word] += 1\n","\n","\n","    maximum_frequncy = max(word_frequencies.values())\n","\n","    for word in word_frequencies.keys():  \n","        word_frequencies[word] = (word_frequencies[word]/maximum_frequncy)\n","\n","\n","    sentence_scores = {}  \n","    for sent in sentence_list:  \n","        for word in nltk.word_tokenize(sent.lower()):\n","            if word in word_frequencies.keys():\n","                if len(sent.split(' ')) < 30:\n","                    if sent not in sentence_scores.keys():\n","                        sentence_scores[sent] = word_frequencies[word]\n","                    else:\n","                        sentence_scores[sent] += word_frequencies[word]\n","\n","\n","    summary_sentences = heapq.nlargest(7, sentence_scores, key=sentence_scores.get)\n","\n","    summary = ' '.join(summary_sentences)\n","    print(\"Original Text::::::::::::\\n\")\n","    print(text_without_removing_dot)\n","    print('\\n\\nSummarized text::::::::\\n')\n","    print(summary)  "],"execution_count":17,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3bPFLH__Pj_","executionInfo":{"status":"ok","timestamp":1604053868655,"user_tz":-330,"elapsed":952,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}},"outputId":"727fb5ce-f49b-44d4-d7e2-866dedd07b4c","colab":{"base_uri":"https://localhost:8080/"}},"source":["generate_summary(df['Original_Text'][0], df['Original_Text'][0])"],"execution_count":19,"outputs":[{"output_type":"stream","text":["Original Text::::::::::::\n","\n","Greetings, Nationstar mortgage contacted me saying that my association no longer had valid flood insurance. I checked with my association and it turns out we no longer needed flood insurance for my condo. I got the required \" Standard Flood Hazard Determination '' document from my condo stating that the condo was no longer in a flood zone. I uploaded the document per Nationstar instructions and also faxed it. A month letter I got another letter. I called and they said they did get the documents but they were backed up and would fix it. I received at least XXXX more letters and called XXXX more times. Each time I was told this would be fixed quickly. Finally they took out an escrow to pay for the flood insurance I did n't need. I called and they showed that part of the insurance had been refunded but not all. Again, they said they would fix.\n","\n","\n","Summarized text::::::::\n","\n","I got the required  Standard Flood Hazard Determination  document from my condo stating that the condo was no longer in a flood zone I checked with my association and it turns out we no longer needed flood insurance for my condo Greetings Nationstar mortgage contacted me saying that my association no longer had valid flood insurance Finally they took out an escrow to pay for the flood insurance I did nt need A month letter I got another letter I uploaded the document per Nationstar instructions and also faxed it I received at least XXXX more letters and called XXXX more times\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ksViBLB59bq4"},"source":["## Upload Data"]},{"cell_type":"markdown","metadata":{"id":"wkCz726v9bq7"},"source":["**Data pipeline (from development to deployment**)\n","\n","![Data pipeline (from development to deployment)](https://miro.medium.com/max/1348/1*Cdp4y1tfMxqoj96o6lUdFg.png)\n","\n","\n","Source:Shoa "]},{"cell_type":"code","metadata":{"id":"e094FThe9bq-","executionInfo":{"status":"aborted","timestamp":1604053694494,"user_tz":-330,"elapsed":1961,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["#load library\n","import os\n","import pandas as pd\n","import numpy as np\n","import gensim\n","from gensim.utils import simple_preprocess\n","from gensim import corpora, models\n","from gensim.parsing.preprocessing import STOPWORDS\n","from nltk.stem import WordNetLemmatizer, SnowballStemmer\n","from nltk.stem.porter import *\n","\n","import datetime\n","import time\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","import nltk"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aQqA5pSQ9bsP"},"source":["## Utils"]},{"cell_type":"code","metadata":{"id":"dQAtXHti9bsS","executionInfo":{"status":"aborted","timestamp":1604053694498,"user_tz":-330,"elapsed":1954,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from collections import Counter\n","from sklearn.metrics import silhouette_score\n","import umap\n","import matplotlib.pyplot as plt\n","from wordcloud import WordCloud\n","from gensim.models.coherencemodel import CoherenceModel\n","import numpy as np\n","import os\n","\n","\n","def get_topic_words(token_lists, labels, k=None):\n","    \"\"\"\n","    get top words within each topic from clustering results\n","    \"\"\"\n","    if k is None:\n","        k = len(np.unique(labels))\n","    topics = ['' for _ in range(k)]\n","    for i, c in enumerate(token_lists):\n","        topics[labels[i]] += (' ' + ' '.join(c))\n","    word_counts = list(map(lambda x: Counter(x.split()).items(), topics))\n","    # get sorted word counts\n","    word_counts = list(map(lambda x: sorted(x, key=lambda x: x[1], reverse=True), word_counts))\n","    # get topics\n","    topics = list(map(lambda x: list(map(lambda x: x[0], x[:10])), word_counts))\n","\n","    return topics\n","\n","def get_coherence(model, token_lists, measure='c_v'):\n","    \"\"\"\n","    Get model coherence from gensim.models.coherencemodel\n","    :param model: Topic_Model object\n","    :param token_lists: token lists of docs\n","    :param topics: topics as top words\n","    :param measure: coherence metrics\n","    :return: coherence score\n","    \"\"\"\n","    if model.method == 'LDA':\n","        cm = CoherenceModel(model=model.ldamodel, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n","                            coherence=measure)\n","    else:\n","        topics = get_topic_words(token_lists, model.cluster_model.labels_)\n","        cm = CoherenceModel(topics=topics, texts=token_lists, corpus=model.corpus, dictionary=model.dictionary,\n","                            coherence=measure)\n","    return cm.get_coherence()\n","\n","def get_silhouette(model):\n","    \"\"\"\n","    Get silhouette score from model\n","    :param model: Topic_Model object\n","    :return: silhouette score\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    lbs = model.cluster_model.labels_\n","    vec = model.vec[model.method]\n","    return silhouette_score(vec, lbs)\n","\n","def plot_proj(embedding, lbs):\n","    \"\"\"\n","    Plot UMAP embeddings\n","    :param embedding: UMAP (or other) embeddings\n","    :param lbs: labels\n","    \"\"\"\n","    n = len(embedding)\n","    counter = Counter(lbs)\n","    for i in range(len(np.unique(lbs))):\n","        plt.plot(embedding[:, 0][lbs == i], embedding[:, 1][lbs == i], '.', alpha=0.5,\n","                 label='cluster {}: {:.2f}%'.format(i, counter[i] / n * 100))\n","    plt.legend(loc = 'best')\n","    plt.grid(color ='grey', linestyle='-',linewidth = 0.25)\n","\n","\n","def visualize(model):\n","    \"\"\"\n","    Visualize the result for the topic model by 2D embedding (UMAP)\n","    :param model: Topic_Model object\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    reducer = umap.UMAP()\n","    print('Calculating UMAP projection ...')\n","    vec_umap = reducer.fit_transform(model.vec[model.method])\n","    print('Calculating UMAP projection. Done!')\n","    plot_proj(vec_umap, model.cluster_model.labels_)\n","    dr = '/content/drive/My Drive/CFPB LDA/{}'.format(model.method, model.id)\n","    if not os.path.exists(dr):\n","        os.makedirs(dr)\n","    plt.savefig('/content/drive/My Drive/CFPB LDA/')\n","\n","def get_wordcloud(model, token_lists, topic):\n","    \"\"\"\n","    Get word cloud of each topic from fitted model\n","    :param model: Topic_Model object\n","    :param sentences: preprocessed sentences from docs\n","    \"\"\"\n","    if model.method == 'LDA':\n","        return\n","    print('Getting wordcloud for topic {} ...'.format(topic))\n","    lbs = model.cluster_model.labels_\n","    tokens = ' '.join([' '.join(_) for _ in np.array(token_lists)[lbs == topic]])\n","\n","    wordcloud = WordCloud(width=800, height=560,\n","                          background_color='white', collocations=False,\n","                          min_font_size=10).generate(tokens)\n","\n","    # plot the WordCloud image\n","    plt.figure(figsize=(8, 5.6), facecolor=None)\n","    plt.imshow(wordcloud)\n","    plt.axis(\"off\")\n","    plt.tight_layout(pad=0)\n","    dr = '/kaggle/working/{}/{}'.format(model.method, model.id)\n","    if not os.path.exists(dr):\n","        os.makedirs(dr)\n","    plt.savefig('/kaggle/working' + '/Topic' + str(topic) + '_wordcloud')\n","    print('Getting wordcloud for topic {}. Done!'.format(topic))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nx3N389h9bsa"},"source":["## Preprocessing "]},{"cell_type":"code","metadata":{"id":"ko2xUsYA9bsc","executionInfo":{"status":"aborted","timestamp":1604053694501,"user_tz":-330,"elapsed":1949,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from stop_words import get_stop_words\n","from nltk.stem.porter import PorterStemmer\n","import re\n","import nltk\n","from nltk.tokenize import word_tokenize\n","from language_detector import detect_language\n","\n","import pkg_resources\n","from symspellpy import SymSpell, Verbosity\n","\n","sym_spell = SymSpell(max_dictionary_edit_distance=3, prefix_length=7)\n","dictionary_path = pkg_resources.resource_filename(\n","    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n","if sym_spell.word_count:\n","    pass\n","else:\n","    sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n","\n","\n","###################################\n","#### sentence level preprocess ####\n","###################################\n","\n","# lowercase + base filter\n","# some basic normalization\n","def f_base(s):\n","    \"\"\"\n","    :param s: string to be processed\n","    :return: processed string: see comments in the source code for more info\n","    \"\"\"\n","    '''# normalization 1: xxxThis is a --> xxx. This is a (missing delimiter)\n","    s = re.sub(r'([a-z])([A-Z])', r'\\1\\. \\2', s)  # before lower case\n","    # normalization 2: lower case\n","    s = s.lower()\n","    # normalization 3: \"&gt\", \"&lt\"\n","    s = re.sub(r'&gt|&lt', ' ', s)\n","    # normalization 4: letter repetition (if more than 2)\n","    s = re.sub(r'([a-z])\\1{2,}', r'\\1', s)\n","    # normalization 5: non-word repetition (if more than 1)\n","    s = re.sub(r'([\\W+])\\1{1,}', r'\\1', s)\n","    # normalization 6: string * as delimiter\n","    s = re.sub(r'\\*|\\W\\*|\\*\\W', '. ', s)\n","    # normalization 7: stuff in parenthesis, assumed to be less informal\n","    s = re.sub(r'\\(.*?\\)', '. ', s)\n","    # normalization 8: xxx[?!]. -- > xxx.\n","    s = re.sub(r'\\W+?\\.', '.', s)\n","    # normalization 9: [.?!] --> [.?!] xxx\n","    s = re.sub(r'(\\.|\\?|!)(\\w)', r'\\1 \\2', s)\n","    # normalization 10: ' ing ', noise text\n","    s = re.sub(r' ing ', ' ', s)\n","    # normalization 11: noise text\n","    s = re.sub(r'product received for free[.| ]', ' ', s)\n","    # normalization 12: phrase repetition\n","    s = re.sub(r'(.{2,}?)\\1{1,}', r'\\1', s)'''\n","\n","    #return s.strip()\n","    return s\n","\n","\n","# language detection\n","def f_lan(s):\n","    \"\"\"\n","    :param s: string to be processed\n","    :return: boolean (s is English)\n","    \"\"\"\n","\n","    # some reviews are actually english but biased toward french\n","    return 1\n","\n","\n","###############################\n","#### word level preprocess ####\n","###############################\n","\n","# filtering out punctuations and numbers\n","def f_punct(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with punct and number filter out\n","    \"\"\"\n","    return [word for word in w_list if word.isalpha()]\n","\n","\n","# selecting nouns\n","def f_noun(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with only nouns selected\n","    \"\"\"\n","    return [word for (word, pos) in nltk.pos_tag(w_list) if pos[:2] == 'NN']\n","\n","\n","# typo correction\n","def f_typo(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with typo fixed by symspell. words with no match up will be dropped\n","    \"\"\"\n","    w_list_fixed = []\n","    for word in w_list:\n","        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=3)\n","        if suggestions:\n","            w_list_fixed.append(suggestions[0].term)\n","        else:\n","            pass\n","            # do word segmentation, deprecated for inefficiency\n","            # w_seg = sym_spell.word_segmentation(phrase=word)\n","            # w_list_fixed.extend(w_seg.corrected_string.split())\n","    return w_list_fixed\n","\n","\n","# stemming if doing word-wise\n","p_stemmer = PorterStemmer()\n","\n","\n","def f_stem(w_list):\n","    \"\"\"\n","    :param w_list: word list to be processed\n","    :return: w_list with stemming\n","    \"\"\"\n","    return [p_stemmer.stem(word) for word in w_list]\n","\n","\n","# filtering out stop words\n","# create English stop words list\n","\n","stop_words = (list(\n","    set(get_stop_words('en'))\n","    |set(get_stop_words('es'))\n","    |set(get_stop_words('de'))\n","    |set(get_stop_words('it'))\n","    |set(get_stop_words('ca'))\n","    #|set(get_stop_words('cy'))\n","    |set(get_stop_words('pt'))\n","    #|set(get_stop_words('tl'))\n","    |set(get_stop_words('pl'))\n","    #|set(get_stop_words('et'))\n","    |set(get_stop_words('da'))\n","    |set(get_stop_words('ru'))\n","    #|set(get_stop_words('so'))\n","    |set(get_stop_words('sv'))\n","    |set(get_stop_words('sk'))\n","    #|set(get_stop_words('cs'))\n","    |set(get_stop_words('nl'))\n","    #|set(get_stop_words('sl'))\n","    #|set(get_stop_words('no'))\n","    #|set(get_stop_words('zh-cn'))\n","))\n","\n","def f_stopw(w_list):\n","    \"\"\"\n","    filtering out stop words\n","    \"\"\"\n","    return [word for word in w_list if word not in stop_words]\n","\n","\n","def preprocess_sent(rw):\n","    \"\"\"\n","    Get sentence level preprocessed data from raw review texts\n","    :param rw: review to be processed\n","    :return: sentence level pre-processed review\n","    \"\"\"\n","    s = f_base(rw)\n","    if not f_lan(s):\n","        return None\n","    return s\n","\n","\n","def preprocess_word(s):\n","    \"\"\"\n","    Get word level preprocessed data from preprocessed sentences\n","    including: remove punctuation, select noun, fix typo, stem, stop_words\n","    :param s: sentence to be processed\n","    :return: word level pre-processed review\n","    \"\"\"\n","    if not s:\n","        return None\n","    w_list = word_tokenize(s)\n","    w_list = f_punct(w_list)\n","    w_list = f_noun(w_list)\n","    w_list = f_typo(w_list)\n","    w_list = f_stem(w_list)\n","    w_list = f_stopw(w_list)\n","\n","    return w_list"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"keGOxoLC9bsn"},"source":["## Autoencoder"]},{"cell_type":"code","metadata":{"id":"PD9rs7Ob9bsp","executionInfo":{"status":"aborted","timestamp":1604053694503,"user_tz":-330,"elapsed":1941,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["import keras\n","from keras.layers import Input, Dense\n","from keras.models import Model\n","from sklearn.model_selection import train_test_split\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","class Autoencoder:\n","    \"\"\"\n","    Autoencoder for learning latent space representation\n","    architecture simplified for only one hidden layer\n","    \"\"\"\n","\n","    def __init__(self, latent_dim=32, activation='relu', epochs=200, batch_size=128):\n","        self.latent_dim = latent_dim\n","        self.activation = activation\n","        self.epochs = epochs\n","        self.batch_size = batch_size\n","        self.autoencoder = None\n","        self.encoder = None\n","        self.decoder = None\n","        self.his = None\n","\n","    def _compile(self, input_dim):\n","        \"\"\"\n","        compile the computational graph\n","        \"\"\"\n","        input_vec = Input(shape=(input_dim,))\n","        encoded = Dense(self.latent_dim, activation=self.activation)(input_vec)\n","        decoded = Dense(input_dim, activation=self.activation)(encoded)\n","        self.autoencoder = Model(input_vec, decoded)\n","        self.encoder = Model(input_vec, encoded)\n","        encoded_input = Input(shape=(self.latent_dim,))\n","        decoder_layer = self.autoencoder.layers[-1]\n","        self.decoder = Model(encoded_input, self.autoencoder.layers[-1](encoded_input))\n","        self.autoencoder.compile(optimizer='adam', loss=keras.losses.mean_squared_error)\n","\n","    def fit(self, X):\n","        if not self.autoencoder:\n","            self._compile(X.shape[1])\n","        X_train, X_test = train_test_split(X)\n","        self.his = self.autoencoder.fit(X_train, X_train,\n","                                        epochs=200,\n","                                        batch_size=128,\n","                                        shuffle=True,\n","                                        validation_data=(X_test, X_test), verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Y58hWmhX9bsx"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"UMkiPUbM9bsz","executionInfo":{"status":"aborted","timestamp":1604053694505,"user_tz":-330,"elapsed":1930,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.cluster import KMeans\n","from gensim import corpora\n","import gensim\n","import numpy as np\n","#from Autoencoder import *\n","#from preprocess import *\n","from datetime import datetime\n","\n","\n","def preprocess(docs, samp_size=None):\n","    \"\"\"\n","    Preprocess the data\n","    \"\"\"\n","    if not samp_size:\n","        samp_size = 100\n","\n","    print('Preprocessing raw texts ...')\n","    n_docs = len(docs)\n","    sentences = []  # sentence level preprocessed\n","    token_lists = []  # word level preprocessed\n","    idx_in = []  # index of sample selected\n","    #     samp = list(range(100))\n","    samp = np.random.choice(n_docs, samp_size)\n","    for i, idx in enumerate(samp):\n","        sentence = preprocess_sent(docs[idx])\n","        token_list = preprocess_word(sentence)\n","        if token_list:\n","            idx_in.append(idx)\n","            sentences.append(sentence)\n","            token_lists.append(token_list)\n","        print('{} %'.format(str(np.round((i + 1) / len(samp) * 100, 2))), end='\\r')\n","    print('Preprocessing raw texts. Done!')\n","    return sentences, token_lists, idx_in\n","\n","\n","# define model object\n","class Topic_Model:\n","    def __init__(self, k=11, method='TFIDF'):\n","        \"\"\"\n","        :param k: number of topics\n","        :param method: method chosen for the topic model\n","        \"\"\"\n","        if method not in {'TFIDF', 'LDA', 'BERT', 'LDA_BERT'}:\n","            raise Exception('Invalid method!')\n","        self.k = k\n","        self.dictionary = None\n","        self.corpus = None\n","        self.np_corpus = None\n","        self.corpus_new = None\n","        self.del_ids = None\n","        #         self.stopwords = None\n","        self.cluster_model = None\n","        self.ldamodel = None\n","        self.vec = {}\n","        self.gamma = 15  # parameter for reletive importance of lda\n","        self.method = method\n","        self.AE = None\n","        self.id = method + '_' + datetime.now().strftime(\"%Y_%m_%d_%H_%M_%S\")\n","\n","    def vectorize(self, sentences, token_lists, method=None):\n","        \"\"\"\n","        Get vecotr representations from selected methods\n","        \"\"\"\n","        # Default method\n","        if method is None:\n","            method = self.method\n","        # turn tokenized documents into a id <-> term dictionary\n","        self.dictionary = corpora.Dictionary(token_lists)      \n","\n","        #Remove unneccessary keywords from token list of corpus. First identify the keywords and then remove them from dictionary and respectively on the corpus vector            \n","        #rmv_words_corpus = ['provide','account','any','them','we','payment','or','company','dispute','you','request','by','an','our','your','will','consumer','due','send','which','over','receive','inform','show','never','add','their','please','owe','now','what','proof','state','amount','late','because','check','since','ask','apply','back','take','mine','notify','only','belong_me','can','proper','there','mail','know','sign','default','eviction','name','additional','debt','accuracy','behind','allege','subsection','regular','manager','instrument_bear','subparagraph','render','mother','content','accordance_with','landlord','door','data_furnisher','exact_same','description','second','completeness','shall','base_upon','absent','ratio','justify','sister','reasonably','operate','see_attachment','regularly','threatening','marry','stopped','bury','morning','pad','operator','displace','ship','mediate','say','contact','letter','need','insurance','so','credit','out','she','also','date','bank','give','up','just','change','increase','recent','shortage','how_much','partial','flood','renewal','periodic','reminder','apology','spread','transition','gap','rudely','flood_zone','midst','maturity','master_policy','document','file','who','case','modification','new','attorney','still','record','correct','case_number','against','action','client','lie','trustee','attached','dual','recast','caliber','favor','relevant','entry','consequence','nominee','execution','character','days','clock','herein','united','pursuit','chat','notwithstanding','concrete','yours','willful','undo','inaction','patiently','hate','tax','try','address','could','work','process','at','copy','free','inquiry','deny','balance','off','all','foreclose','make','into','investigation','get','husband','number','fraud','personal','able','rate','want','way','access','submit','plan','month','form','assist','act','offer','county','initiate','rent','trust','site','lower','discover','live_at','understand','view','privacy','consent','employee','additionally','fill_out','best','residence','connect','obvious','pin','insert','conference','eligible','go','defer','back_onto','retrieve','annual','motion','expose','unwanted','carry','crime','taxpayer','vehicle','exempt','divide','racketeering','jumbo','imply','technical','reassess','bond','subpoena','hedge','gon_na','husband_die','star','scary','proven','bubble','spirit','excite','earner','margin','he','fee','but','again','close','application','time','bureau','these','help','transfer','list','after','keep','lender','even','more','appraisal','representative','attach','him','per','return','order','his','nothing','none','deed','particular','packet','pa','comment','age','bogus','identification','child','draft','real','waive','might','sent_certificate','hire','legitimately','treat','together','contractual','sum','stole','desist','world','mi','harassment','direction','imagine','counselor','corporate','priority','business_practice','sheriff','remodification','reassign','arm','rush','combine','farm','faster','ethic','annuity','stall_tactic','declination','click','administer','offset','teacher','folder','unlike','dissatisfy','naturally','tell','when','card','should','would','score','service','escrow','negative','one','bill','find','continue','issue','original','refuse','multiple','response','term','value','private','schedule','refund','power','release','post','reflect','average','press','front','rep','meanwhile','watch','karma','user','frustration','certificate','intentionally','accident','enforce','deduct','evidently','length','percent','green','time_fashion','newly','manual','ie','float','pro','anxious','frankly','assessed','pay','sever','verify','incorrect','creditor','open','false','delete','use','call','regard','phone','another','previous','speak','total','current','satisfy','estimate','submission','understanding','town','stuff','enclose','generic','photo','electronic','sincerely','advance','specify','rather_than','doc','pass','exemption','scan','hud','disburse','shortfall','wrongful','accumulate','minus','gift','realistic','contention','recur','column','girl','flip','sloppy','caught','continuance','buck','house','sell','buy','about','through','her','loan','update','right','same','start','place','own','result','resolve','down','title','low','away','happen','spoke','price','benefit','represent','firm','unit','got','messenger','duplex','man','neighbor','vacant','incident','per_month','incur','urgent','opinion','reinsert','extension','prefer','friend','borrow','perfect','sheet','street','water','brother','sometime','reward','differ','belonging','occupy','floor','yard','tree','ex_husband','marketing','dog','score_suffer','wide','liar','shape','predator','driveway','drug','redemption','cash_key','wow','heat','kickback','recorder','item','inaccurate','charge','interest','money','fund','law','immediately','court','principle','put','must','furnish','policy','wrong','lien','source','figure','treatment','challenge','contractor','physical','any_kind','certification','certify','suspense','draw','relief','verifiable','deem','statue','adjudication','wit','disturb','entirely','nonexistent','eye','frozen','affair','inspector','auto','fay','revenue','storm','forced','teller','index','enough_cover','recalculate','payee','septic','occurrence','unhelpful','manually','spoken','review','require','approve','report','borrower','advise','within','delay','rule','guideline','appraiser','builder','accurately','tip','evade','chart','exclude','duration','directive','conditional','origin','noncompliance','rebate','remit','procure','reform','questionnaire','accent','foreclosure','short_sale','complain','complete','notice','other','income','sale','office','trial','appeal','decision','conduct','specialist','exhibit','forbearance','mediation','forge','eligibility','exercise','article','blank','hereby','format','foreign','harp','limitation','civil_code','commissioner','writer','log','facially_complete']\n","        rmv_words_corpus = ['inform','receive','mortgage','number','send','work','short','make','mother','account','contact','tell','proper','request','customer','wife','say','company','family','refinance','basically','date','value','unit','provide','check','job','record','continue','submit','roof','apply','complain','trust','new','remove','approve','land','try','increase','ask','husband','sign','high','right','correct','program','need','situation','term','chapter','line','search','bank','shortage','personal','cause','original','buy','trial','want','replace','case','loss','low','demand','negative','just','flood','representative','hardship','attempt','dispute','apartment','live','child','action','current','history','know','commitment','homeowner','response','speak','life','assignment','refuse','perform','analysis','owe','feel','complete','add','hire','county','additional','borrower','use','pull','divorce','option','require','start','place','write','question','copy','consumer','final']\n","        # assume the word 'b' is to be deleted, put its id in a variable\n","        self.del_ids = [k for k,v in self.dictionary.items() if v in rmv_words_corpus]\n","\n","        # remove unwanted word ids from the dictionary in place\n","        self.dictionary.filter_tokens(bad_ids=self.del_ids)\n","        # convert tokenized documents into a document-term matrix\n","        self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","        gensim.corpora.MmCorpus.serialize('Mortgage_CorpusAll.mm', self.corpus)        \n","        '''self.corpus = gensim.corpora.MmCorpus('Mortgage_CorpusAll.mm')\n","        # convert corpus to a dense array, transpose because by default documents would be columns\n","        self.np_corpus = gensim.matutils.corpus2dense(self.corpus, self.corpus.num_terms, num_docs=self.corpus.num_docs).T\n","        # delete columns for specified tokens, transpose back afterwards\n","        self.np_corpus = np.delete(self.np_corpus, self.del_ids, 1).T\n","        # convert array to corpus\n","        self.corpus_new = gensim.matutils.Dense2Corpus(self.np_corpus)'''\n","        \n","\n","        if method == 'TFIDF':\n","            print('Getting vector representations for TF-IDF ...')\n","            tfidf = TfidfVectorizer()\n","            vec = tfidf.fit_transform(sentences)\n","            print('Getting vector representations for TF-IDF. Done!')\n","            return vec\n","\n","        elif method == 'LDA':\n","            print('Getting vector representations for LDA ...')\n","            if not self.ldamodel:\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n","                                                                passes=20)\n","                self.ldamodel.save('lda_train4.model')\n","\n","            def get_vec_lda(model, corpus, k):\n","                \"\"\"\n","                Get the LDA vector representation (probabilistic topic assignments for all documents)\n","                :return: vec_lda with dimension: (n_doc * n_topic)\n","                \"\"\"\n","                n_doc = len(corpus)\n","                vec_lda = np.zeros((n_doc, k))\n","                for i in range(n_doc):\n","                    # get the distribution for the i-th document in corpus\n","                    for topic, prob in model.get_document_topics(corpus[i]):\n","                        vec_lda[i, topic] = prob                \n","                return vec_lda\n","\n","            vec = get_vec_lda(self.ldamodel, self.corpus, self.k)\n","            pickle.dump(vec, open(\"Mortgage_train_vec.pkl\", \"wb\" ))\n","            print('Getting vector representations for LDA. Done!')\n","            return vec\n","\n","        elif method == 'BERT':\n","            '''## Load required packages for FinBERT\n","            from transformers import AutoTokenizer, AutoModelForSequenceClassification\n","            import torch\n","            import numpy as np\n","\n","\n","            %reload_ext watermark\n","            %watermark -v -p numpy,torch,transformers\n","            print('Getting vector representations for BERT ...')\n","            tokenizer = AutoTokenizer.from_pretrained(\"ipuneetrathore/bert-base-cased-finetuned-finBERT\")\n","            model = AutoModelForSequenceClassification.from_pretrained(\"ipuneetrathore/bert-base-cased-finetuned-finBERT\")'''\n","            print('Getting vector representations for BERT ...')\n","            from sentence_transformers import SentenceTransformer\n","            model = SentenceTransformer('bert-base-nli-max-tokens')\n","            vec = np.array(model.encode(sentences, show_progress_bar=True))\n","            print('Getting vector representations for BERT. Done!')\n","            return vec\n","            vec = np.array(model.encode(sentences, show_progress_bar=True))\n","            print('Getting vector representations for BERT. Done!')\n","            return vec\n","\n","             \n","        elif method == 'LDA_BERT':\n","        #else:\n","            vec_lda = self.vectorize(sentences, token_lists, method='LDA')\n","            vec_bert = self.vectorize(sentences, token_lists, method='BERT')\n","            vec_ldabert = np.c_[vec_lda * self.gamma, vec_bert]\n","            self.vec['LDA_BERT_FULL'] = vec_ldabert\n","            if not self.AE:\n","                self.AE = Autoencoder()\n","                print('Fitting Autoencoder ...')\n","                self.AE.fit(vec_ldabert)\n","                print('Fitting Autoencoder Done!')\n","            vec = self.AE.encoder.predict(vec_ldabert)\n","            return vec\n","\n","    def fit(self, sentences, token_lists, method=None, m_clustering=None):\n","        \"\"\"\n","        Fit the topic model for selected method given the preprocessed data\n","        :docs: list of documents, each doc is preprocessed as tokens\n","        :return:\n","        \"\"\"\n","        # Default method\n","        if method is None:\n","            method = self.method\n","        # Default clustering method\n","        if m_clustering is None:\n","            m_clustering = KMeans\n","\n","        # turn tokenized documents into a id <-> term dictionary\n","        if not self.dictionary:\n","            self.dictionary = corpora.Dictionary(token_lists)           \n","\n","            #Remove unneccessary keywords from token list of corpus. First identify the keywords and then remove them from dictionary and respectively on the corpus vector            \n","            #rmv_words_corpus = ['provide','account','any','them','we','payment','or','company','dispute','you','request','by','an','our','your','will','consumer','due','send','which','over','receive','inform','show','never','add','their','please','owe','now','what','proof','state','our','amount','late','because','check','since','ask','never','apply','any','back','take','mine','notify','only','you','belong_me','can','proper','there','please','mail','know','sign','default','eviction','name','additional','debt','accuracy','behind','allege','subsection','regular','manager','instrument_bear','subparagraph','render','mother','content','accordance_with','landlord','door','data_furnisher','exact_same','description','second','completeness','shall','base_upon','absent','ratio','justify','sister','reasonably','operate','see_attachment','regularly','threatening','marry','stopped','bury','morning','pad','operator','displace','ship','mediate','account','request','our','inform','say','because','amount','contact','letter','need','insurance','so','any','due','will','credit','company','take','now','over','out','she','also','ask','date','bank','back','provide','only','give','up','just','add','change','increase','recent','shortage','how_much','partial','flood','renewal','periodic','reminder','apology','spread','transition','gap','rudely','flood_zone','midst','maturity','master_policy','state','document','file','any','what','request','late','can','who','account','case','modification','need','new','show','attorney','because','apply','ask','still','amount','never','only','record','will','date','you','your','correct','case_number','against','action','client','lie','trustee','attached','dual','recast','caliber','favor','relevant','entry','consequence','nominee','execution','character','days','clock','herein','united','pursuit','chat','notwithstanding','concrete','yours','willful','undo','inaction','patiently','hate','our','modification','company','request','tax','receive','say','now','an','try','dispute','address','since','so','out','by','ask','can','their','take','could','letter','what','them','proper','will','document','work','never','process','your','amount','your','up','back','over','account','there','you','due','at','sign','copy','send','free','file','inquiry','company','letter','credit','deny','balance','off','all','provide','foreclose','make','mail','can','into','which','investigation','contact','there','get','or','husband','number','fraud','personal','able','rate','since','want','way','access','submit','plan','month','form','assist','act','offer','county','initiate','rent','trust','site','lower','discover','live_at','understand','view','privacy','consent','employee','additionally','fill_out','best','residence','connect','obvious','pin','insert','conference','eligible','go','defer','back_onto','retrieve','annual','motion','expose','unwanted','carry','consequence','crime','taxpayer','vehicle','exempt','divide','racketeering','jumbo','imply','technical','reassess','bond','subpoena','hedge','gon_na','husband_die','star','scary','proven','bubble','spirit','excite','earner','margin','all','them','he','get','could','fee','so','dispute','send','letter','back','payment','which','but','or','only','request','modification','ask','try','now','give','our','process','can','there','never','state','again','say','you','there','by','receive','account','what','can','close','copy','need','submit','because','work','their','document','application','time','take','so','also','at','bureau','add','correct','file','these','company','please','help','transfer','list','after','want','still','keep','inquiry','lender','since','off','even','back','know','more','appraisal','representative','attach','him','per','return','order','his','nothing','access','none','deed','particular','packet','mother','pa','comment','age','bogus','identification','child','draft','real','waive','might','sent_certificate','hire','legitimately','second','treat','together','contractual','sum','stole','desist','world','mi','harassment','direction','imagine','counselor','corporate','caliber','priority','business_practice','sheriff','remodification','mi','reassign','arm','rush','combine','farm','faster','ethic','annuity','stall_tactic','declination','click','administer','offset','teacher','folder','unlike','dissatisfy','naturally','an','there','them','dispute','payment','our','you','get','all','can','need','receive','tell','balance','try','or','their','add','when','will','card','file','so','show','time','should','company','contact','because','only','but','would','score','service','account','escrow','close','can','request','amount','up','could','now','close','show','negative','document','help','one','new','state','check','amount','bill','modification','so','due','find','say','insurance','what','letter','will','because','date','company','there','out','should','could','never','still','ask','also','continue','file','issue','debt','off','original','refuse','vehicle','multiple','response','term','value','private','schedule','refund','power','release','post','reflect','county','average','press','front','rep','meanwhile','watch','karma','user','frustration','certificate','intentionally','accident','enforce','deduct','evidently','length','percent','green','time_fashion','newly','manual','ie','float','pro','anxious','frankly','assessed','pay','sever','tax','account','dispute','payment','close','by','verify','amount','incorrect','file','you','creditor','date','check','all','name','company','correct','when','open','document','there','so','add','false','now','which','need','an','can','our','show','delete','contact','time','use','call','but','show','regard','amount','phone','provide','out','company','will','another','previous','speak','correct','total','current','refund','satisfy','estimate','submission','understanding','town','stuff','enclose','manual','generic','photo','electronic','sincerely','advance','specify','rather_than','doc','pass','exemption','scan','card','hud','disburse','shortfall','wrongful','accumulate','minus','gift','realistic','contention','recur','column','girl','flip','sloppy','caught','continuance','buck','she','proper','company','house','now','file','say','can','ask','so','send','never','all','because','payment','any','sell','also','call','what','inform','take','will','score','never','their','still','which','contact','card','try','buy','over','about','letter','need','state','or','since','also','make','help','receive','show','through','by','add','her','know','but','get','loan','them','update','time','close','an','right','same','contact','start','he','place','would','about','name','own','result','address','resolve','him','down','his','title','low','away','happen','spoke','price','benefit','represent','firm','unit','got','messenger','duplex','man','neighbor','vacant','incident','per_month','incur','urgent','opinion','reinsert','extension','prefer','friend','borrow','perfect','sheet','street','water','brother','sometime','reward','differ','belonging','occupy','floor','yard','tree','ex_husband','marketing','dog','score_suffer','wide','liar','shape','predator','driveway','drug','redemption','cash_key','wow','heat','kickback','recorder','dispute','file','or','verify','our','say','them','by','request','account','these','item','bank','payment','consumer','amount','company','all','check','which','any','delete','state','so','could','because','send','add','bureau','you','but','letter','provide','modification','inaccurate','will','because','their','receive','also','over','document','fraud','never','document','investigation','ask','charge','escrow','date','now','she','still','back','state','request','take','can','list','there','give','new','interest','money','late','apply','refuse','issue','fund','law','right','insurance','immediately','even','court','correct','bill','principle','owe','put','must','proof','behind','furnish','policy','wrong','lien','source','figure','treatment','challenge','contractor','physical','any_kind','certification','certify','suspense','deduct','draw','relief','verifiable','deem','length','statue','adjudication','wit','disturb','entirely','nonexistent','eye','frozen','affair','inspector','auto','fay','revenue','storm','forced','teller','index','enough_cover','recalculate','payee','septic','occurrence','unhelpful','manually','spoken','our','letter','request','state','inform','process','document','account','provide','offer','bank','so','review','contact','modification','require','any','submit','there','back','close','approve','due','date','because','amount','say','application','need','also','report','borrower','add','return','response','attorney','advise','regard','within','term','must','per','delay','rule','guideline','appraiser','builder','accurately','tip','evade','periodic','chart','exclude','duration','directive','conditional','origin','noncompliance','rebate','remit','procure','reform','questionnaire','verifiable','accent','our','document','modification','you','inform','he','request','proper','say','foreclosure','account','bank','any','provide','ask','never','state','letter','process','there','so','will','date','house','can','because','file','give','know','about','short_sale','submit','deny','review','approve','complain','application','complete','your','notice','other','income','sale','personal','law','office','trial','appeal','decision','conduct','specialist','exhibit','forbearance','mediation','forge','eligibility','exercise','article','blank','hereby','format','foreign','harp','limitation','civil_code','commissioner','writer','log','facially_complete']\n","            rmv_words_corpus = ['inform','receive','mortgage','number','send','work','short','make','mother','account','contact','tell','proper','request','customer','wife','say','company','family','refinance','basically','date','value','unit','provide','check','job','record','continue','submit','roof','apply','complain','trust','new','remove','approve','land','try','increase','ask','husband','sign','high','right','correct','program','need','situation','term','chapter','line','search','bank','shortage','personal','cause','original','buy','trial','want','replace','case','loss','low','demand','negative','just','flood','representative','hardship','attempt','dispute','apartment','live','child','action','current','history','know','commitment','homeowner','response','speak','life','assignment','refuse','perform','analysis','owe','feel','complete','add','hire','county','additional','borrower','use','pull','divorce','option','require','start','place','write','question','copy','consumer','final']\n","            # assume the word 'b' is to be deleted, put its id in a variable\n","            self.del_ids = [k for k,v in self.dictionary.items() if v in rmv_words_corpus]\n","\n","            # remove unwanted word ids from the dictionary in place\n","            self.dictionary.filter_tokens(bad_ids=self.del_ids)\n","            # convert tokenized documents into a document-term matrix\n","            self.corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","            gensim.corpora.MmCorpus.serialize('Mortgage_CorpusAll_Fit.mm', self.corpus)\n","            '''self.corpus = gensim.corpora.MmCorpus('Mortgage_CorpusAll.mm')\n","            # convert corpus to a dense array, transpose because by default documents would be columns\n","            self.np_corpus = gensim.matutils.corpus2dense(self.corpus, self.corpus.num_terms, num_docs=self.corpus.num_docs).T\n","            # delete columns for specified tokens, transpose back afterwards\n","            self.np_corpus = np.delete(self.np_corpus, self.del_ids, 1).T\n","            # convert array to corpus\n","            self.corpus = gensim.matutils.Dense2Corpus(self.np_corpus)'''\n","        \n","        ####################################################\n","        #### Getting ldamodel or vector representations ####\n","        ####################################################\n","\n","        if method == 'LDA':\n","            if not self.ldamodel:\n","                print('Fitting LDA ...')\n","                self.ldamodel = gensim.models.ldamodel.LdaModel(self.corpus, num_topics=self.k, id2word=self.dictionary,\n","                                                                passes=20)\n","                print('Fitting LDA Done!')\n","        else:\n","            print('Clustering embeddings ...')\n","            self.cluster_model = m_clustering(self.k)\n","            self.vec[method] = self.vectorize(sentences, token_lists, method)\n","            self.cluster_model.fit(self.vec[method])\n","            print('Clustering embeddings. Done!')\n","\n","    def predict(self, sentences, token_lists, out_of_sample=None):\n","        \"\"\"\n","        Predict topics for new_documents\n","        \"\"\"\n","        # Default as False\n","        out_of_sample = out_of_sample is not None\n","\n","        if out_of_sample:\n","            corpus = [self.dictionary.doc2bow(text) for text in token_lists]\n","            if self.method != 'LDA':\n","                vec = self.vectorize(sentences, token_lists)\n","                print(vec)\n","        else:\n","            corpus = corpus\n","            vec = self.vec.get(self.method, None)\n","\n","        if self.method == \"LDA\":\n","            lbs = np.array(list(map(lambda x: sorted(self.ldamodel.get_document_topics(x),\n","                                                     key=lambda x: x[1], reverse=True)[0][0],\n","                                    corpus)))\n","        else:\n","            lbs = self.cluster_model.predict(vec)\n","        return lbs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qV8PvGEu9bs7"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"J7Jc0BBN9bs9","executionInfo":{"status":"aborted","timestamp":1604053694507,"user_tz":-330,"elapsed":1919,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["#from model import *\n","#from utils import *\n","\n","import pandas as pd\n","import pickle\n","import matplotlib.pyplot as plt\n","# Import spacy for lemmatization\n","import spacy\n","\n","import warnings\n","warnings.filterwarnings('ignore', category=Warning)\n","\n","import argparse\n","\n","#def model(): #:if __name__ == '__main__':\n","\n","def main():\n","    \n","    \n","    #method = \"TFIDF\"\n","    #method = \"LDA\"\n","    #method = \"BERT\"    \n","    method = \"LDA_BERT\"\n","    samp_size = 10000\n","    ntopic = 7\n","    \n","    #parser = argparse.ArgumentParser(description='contextual_topic_identification tm_test:1.0')\n","\n","    #parser.add_argument('--fpath', default='/kaggle/working/train.csv')\n","    #parser.add_argument('--ntopic', default=10,)\n","    #parser.add_argument('--method', default='TFIDF')\n","    #parser.add_argument('--samp_size', default=20500)\n","    \n","    #args = parser.parse_args()\n","    complain_data= pd.read_excel(r'/content/drive/My Drive/CFPB LDA/LDA Output Document to topic.xlsx')\n","    data_train = complain_data['Cleansed_Text'].head(10)   \n","    sentences, token_lists, idx_in = preprocess(data_train, samp_size=samp_size)\n","    # Define the topic model object\n","    #tm = Topic_Model(k = 10), method = TFIDF)\n","    tm = Topic_Model(k = ntopic, method = method)\n","    # Fit the topic model by chosen method\n","    tm.fit(sentences, token_lists)\n","\n","    print('Coherence:', get_coherence(tm, token_lists, 'c_v'))\n","    print('Silhouette Score:', get_silhouette(tm))\n","    # visualize and save img\n","    visualize(tm)\n","    for i in range(tm.k):\n","        get_wordcloud(tm, token_lists, i)   "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mbqTgust9btG","executionInfo":{"status":"aborted","timestamp":1604053694509,"user_tz":-330,"elapsed":1912,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["main()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r51xLP9_yXWG","executionInfo":{"status":"aborted","timestamp":1604053694510,"user_tz":-330,"elapsed":1903,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["tm.predict(sentences_test, token_lists_test, corpus)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1CQwTDOzUjI1","executionInfo":{"status":"aborted","timestamp":1604053694512,"user_tz":-330,"elapsed":1894,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["complain_data_test= pd.read_csv(r'/content/drive/My Drive/CFPB LDA/Mortgage_complaints_text_train.csv')\n","print(complain_data_test.columns)\n","data_test = complain_data_test['Complain_text']\n","samp_size = 10000    \n","sentences_test, token_lists_test, idx_in_test = preprocess(data_test, samp_size=samp_size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TTtnNYjojnZ7","executionInfo":{"status":"aborted","timestamp":1604053694514,"user_tz":-330,"elapsed":1883,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["# Import spacy for lemmatization\n","import spacy\n","# Create a list of all words in the dataset\n","def sent_to_words(sentences):\n","    for sentence in sentences:\n","        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))  # deacc=True removes punctuations\n","        \n","data_words = list(sent_to_words(data_test))\n","\n","def make_bigrams(texts):\n","    return [bigram_mod[doc] for doc in texts]\n","def make_trigrams(texts):\n","    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n","def lemmatization(texts, \n","                  allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n","    texts_out = []\n","    for sent in texts:\n","        doc = nlp(\" \".join(sent)) \n","        texts_out.append([token.lemma_ for token in doc \n","                          if token.pos_ in allowed_postags])\n","    return texts_out\n","\n","# python3 -m spacy download en\n","nlp = spacy.load('en', disable=['parser', 'ner'])\n","\n","# Do lemmatization keeping only noun, adj, vb, adv\n","data_lemmatized = lemmatization(data_words , \n","                  allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CvsgJrBzlpWI","executionInfo":{"status":"aborted","timestamp":1604053694515,"user_tz":-330,"elapsed":1873,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["# Create Dictionary\n","id2word = corpora.Dictionary(data_lemmatized)\n","# Create Corpus\n","texts = data_lemmatized\n","# Term Document Frequency\n","corpus = [id2word.doc2bow(text) for text in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mV2gSh7alz4G","executionInfo":{"status":"aborted","timestamp":1604053694517,"user_tz":-330,"elapsed":1862,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["lbs = np.array(list(map(lambda x: sorted(ldamodel.get_document_topics(x),key=lambda x: x[1], reverse=True)[0][0],corpus)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jo3vjwyEYbUA","executionInfo":{"status":"aborted","timestamp":1604053694518,"user_tz":-330,"elapsed":1853,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["ntopic = 11\n","method = \"LDA\"\n","tm = Topic_Model(k = ntopic, method = method)\n","tm.predict(sentences_test, token_lists_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-JjumKsrZBmY","executionInfo":{"status":"aborted","timestamp":1604053694519,"user_tz":-330,"elapsed":1844,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["corpus = [dictionary.doc2bow(text) for text in token_lists_test]\n","lbs = np.array(list(map(lambda x: sorted(ldamodel.get_document_topics(x),key=lambda x: x[1], reverse=True)[0][0],corpus)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lqeONpVjaA1Q","executionInfo":{"status":"aborted","timestamp":1604053694520,"user_tz":-330,"elapsed":1834,"user":{"displayName":"KOUSHIK JANA","photoUrl":"","userId":"05214550295711951484"}}},"source":["# Create Dictionary\n","id2word = corpora.Dictionary(token_lists_test)\n","# Create Corpus\n","texts = sentences_test\n","# Term Document Frequency\n","corpus = [id2word.doc2bow(text) for text in texts]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lnFeq-fL9btO"},"source":["### TODO:\n","\n","* Implement models.ldamulticore – parallelized Latent Dirichlet Allocation using all CPU cores to parallelize and speed up model training.\n","* Switch from BERT/RoBERTa to SciBERT, BART, and or other models. "]}]}